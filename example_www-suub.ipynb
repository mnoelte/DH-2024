{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6be654-0282-438f-ae5e-876292cbe7c8",
   "metadata": {},
   "source": [
    "# Prepare script; read web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ab6508-3db4-4ae4-bf33-0e66639dd5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the webpage\n",
    "url = \"https://www.suub.uni-bremen.de/ueber-uns/neues-aus-der-suub/\"\n",
    "\n",
    "# Send a request to the webpage\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141451e4-9c3e-458b-8cee-54870100574c",
   "metadata": {},
   "source": [
    "### The list of news headlines and the first lines of the blogposts are our target.\n",
    "\n",
    "### Dismantel the code cell and create single python cells for\n",
    "- find a h2 heading; restrict the output to the text (what happens without the strip=True arguement of get_text?)\n",
    "- find an arbitrary div tag\n",
    "- find a div tag with class \"blogposts_item_text\" (Why am I referring to \"blogposts_item_text\" and why is this class important?)\n",
    "- find all h2 headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3455db55-723d-40ae-983e-51a30ba8f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find a h2 heading; restrict the output to the text\n",
    "soup.fi..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef2ffc7-5267-4a25-82ac-bbe25d0dea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find an arbitrary div tag\n",
    "soup.fin..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7ef15e-6d6d-4ac4-a513-3f67cf295fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find a div tag with class \"blogposts_item_text\"\n",
    "soup.find(..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea809b5-9389-42a1-abf4-fb0ceea215e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find a div tag with class \"blogposts_item_text\"\n",
    "soup.find(..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc2f7df-1a64-444e-b3b0-12e7b86d2344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all h2 headings\n",
    "soup.fi..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb0380c-4580-4400-aca8-6142e8022c69",
   "metadata": {},
   "source": [
    "## find all div tags with class_=\"blogposts_item_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccfb070-0cf3-4224-83e1-6ef0d701bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and print all h2 headlines and their corresponding div texts\n",
    "for post in soup.find_all(\"div\", class_=\"blogposts_item_text\"):\n",
    "    headline = post.find_previous(\"h2\").get_text(strip=True) if post.find_previous(\"h2\") else \"No headline found\"\n",
    "    text = post.get_text(strip=True)\n",
    "    print(f\"Headline: {headline}\")\n",
    "    print(f\"Text: {text}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eddc012-68ca-438c-b57e-d8e05ea453bb",
   "metadata": {},
   "source": [
    "# Now with pagination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce9aa7-0058-44a4-b2ae-0cadf9b355de",
   "metadata": {},
   "source": [
    "### doesn't work because of '?page={page_number}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b74974-9599-4b3d-a15d-97b54b7f469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Base URL of the page with pagination parameter\n",
    "base_url = \"https://www.suub.uni-bremen.de/ueber-uns/neues-aus-der-suub/\"\n",
    "page_number = 1\n",
    "results = []\n",
    "\n",
    "for i in range(10):  # restricted to 10 pages\n",
    "    # Construct the URL with the page number\n",
    "    url = f\"{base_url}?page={page_number}\"\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Find the blog post items\n",
    "    posts = soup.find_all(\"div\", class_=\"blogposts_item_text\")\n",
    "    if not posts:  # Stop if there are no more posts\n",
    "        break\n",
    "    \n",
    "    for post in posts:\n",
    "        headline = post.find_previous(\"h2\").get_text(strip=True) if post.find_previous(\"h2\") else \"No headline found\"\n",
    "        text = post.get_text(strip=True)\n",
    "        results.append(f\"Headline: {headline}\\nText: {text}\\n\")\n",
    "\n",
    "    page_number += 1  # Go to the next page\n",
    "\n",
    "# Write the results to a text file\n",
    "with open(\"news-suub.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.writelines(results)\n",
    "\n",
    "print(\"Scraping completed and saved to news-suub.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8fddcd-1760-4eb4-aa64-ac4f25848305",
   "metadata": {},
   "source": [
    "### Now a pagination scraper with the correct \"/page_number/\"  (restricted to 50 pages - this goes back to April 2017) and creation of a file news-suub.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8167ce6b-6716-4cf2-93c8-f0bd5fe83714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Base URL of the page with pagination structure\n",
    "base_url = \"https://www.suub.uni-bremen.de/ueber-uns/neues-aus-der-suub/\"\n",
    "page_number = 1\n",
    "results = []\n",
    "\n",
    "for i in range(50):\n",
    "    # Construct the URL with the page number\n",
    "    url = f\"{base_url}{page_number}/\"\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Find the blog post items\n",
    "    posts = soup.find_all(\"div\", class_=\"blogposts_item_text\")\n",
    "    if not posts:  # Stop if there are no more posts\n",
    "        break\n",
    "    \n",
    "    for post in posts:\n",
    "        headline = post.find_previous(\"h2\").get_text(strip=True) if post.find_previous(\"h2\") else \"No headline found\"\n",
    "        text = post.get_text(strip=True)\n",
    "        results.append(f\"Headline: {headline}\\nText: {text}\\n\\n\")\n",
    "\n",
    "    page_number += 1  # Go to the next page\n",
    "\n",
    "# Write the results to a text file\n",
    "with open(\"news-suub.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.writelines(results)\n",
    "\n",
    "print(\"Scraping completed and saved to news-suub.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
