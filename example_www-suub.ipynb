{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6be654-0282-438f-ae5e-876292cbe7c8",
   "metadata": {},
   "source": [
    "# prepare script; read web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ab6508-3db4-4ae4-bf33-0e66639dd5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the webpage\n",
    "url = \"https://www.suub.uni-bremen.de/ueber-uns/neues-aus-der-suub/\"\n",
    "\n",
    "# Send a request to the webpage\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb0380c-4580-4400-aca8-6142e8022c69",
   "metadata": {},
   "source": [
    "## find all div tags with class_=\"blogposts_item_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccfb070-0cf3-4224-83e1-6ef0d701bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and print all h2 headlines and their corresponding div texts\n",
    "for post in soup.find_all(\"div\", class_=\"blogposts_item_text\"):\n",
    "    headline = post.find_previous(\"h2\").get_text(strip=True) if post.find_previous(\"h2\") else \"No headline found\"\n",
    "    text = post.get_text(strip=True)\n",
    "    print(f\"Headline: {headline}\")\n",
    "    print(f\"Text: {text}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eddc012-68ca-438c-b57e-d8e05ea453bb",
   "metadata": {},
   "source": [
    "# Now with pagination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce9aa7-0058-44a4-b2ae-0cadf9b355de",
   "metadata": {},
   "source": [
    "### doesn't work because of '?page={page_number}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b74974-9599-4b3d-a15d-97b54b7f469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Base URL of the page with pagination parameter\n",
    "base_url = \"https://www.suub.uni-bremen.de/ueber-uns/neues-aus-der-suub/\"\n",
    "page_number = 1\n",
    "results = []\n",
    "\n",
    "for i in range(10):\n",
    "    # Construct the URL with the page number\n",
    "    url = f\"{base_url}?page={page_number}\"\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Find the blog post items\n",
    "    posts = soup.find_all(\"div\", class_=\"blogposts_item_text\")\n",
    "    if not posts:  # Stop if there are no more posts\n",
    "        break\n",
    "    \n",
    "    for post in posts:\n",
    "        headline = post.find_previous(\"h2\").get_text(strip=True) if post.find_previous(\"h2\") else \"No headline found\"\n",
    "        text = post.get_text(strip=True)\n",
    "        results.append(f\"Headline: {headline}\\nText: {text}\\n\")\n",
    "\n",
    "    page_number += 1  # Go to the next page\n",
    "\n",
    "# Write the results to a text file\n",
    "with open(\"news-suub.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.writelines(results)\n",
    "\n",
    "print(\"Scraping completed and saved to news-suub.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8fddcd-1760-4eb4-aa64-ac4f25848305",
   "metadata": {},
   "source": [
    "### now with the correct \"/page_number/\"   (restricted to 50 pages - this goes back to April 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8167ce6b-6716-4cf2-93c8-f0bd5fe83714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Base URL of the page with pagination structure\n",
    "base_url = \"https://www.suub.uni-bremen.de/ueber-uns/neues-aus-der-suub/\"\n",
    "page_number = 1\n",
    "results = []\n",
    "\n",
    "for i in range(50):\n",
    "    # Construct the URL with the page number\n",
    "    url = f\"{base_url}{page_number}/\"\n",
    "    print(url)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # Find the blog post items\n",
    "    posts = soup.find_all(\"div\", class_=\"blogposts_item_text\")\n",
    "    if not posts:  # Stop if there are no more posts\n",
    "        break\n",
    "    \n",
    "    for post in posts:\n",
    "        headline = post.find_previous(\"h2\").get_text(strip=True) if post.find_previous(\"h2\") else \"No headline found\"\n",
    "        text = post.get_text(strip=True)\n",
    "        results.append(f\"Headline: {headline}\\nText: {text}\\n\\n\")\n",
    "\n",
    "    page_number += 1  # Go to the next page\n",
    "\n",
    "# Write the results to a text file\n",
    "with open(\"news-suub.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.writelines(results)\n",
    "\n",
    "print(\"Scraping completed and saved to news-suub.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
